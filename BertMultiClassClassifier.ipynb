{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARINC Fingerprinting BERT Single Class Classifier\n",
    "\n",
    "//\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import related libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chriz/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "'''Train with PyTorch.'''\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.utils.data as data\n",
    "\n",
    "# BERT Related Libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring machine learning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Parameters\n",
    "lr = 1e-2\n",
    "epoch = 10\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Classes:  ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of Unique Class:  20\n",
      "=======\n",
      "(11314,)\n",
      "(11314,)\n",
      "=======\n",
      "/Users/chriz/scikit_learn_data/20news_home/20news-bydate-train/rec.autos/102994\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Download News Group Dataset from SciKit\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Print Available Classes\n",
    "print(\"Available Classes: \", list(newsgroups_train.target_names))\n",
    "num_labels = len(list(newsgroups_train.target_names))\n",
    "print(\"Number of Unique Class: \", num_labels)\n",
    "print(\"=======\")\n",
    "\n",
    "# Print Data Count\n",
    "print(newsgroups_train.filenames.shape)\n",
    "print(newsgroups_train.target.shape)\n",
    "print(\"=======\")\n",
    "\n",
    "# Print One Data Example\n",
    "print(newsgroups_train.filenames[0])\n",
    "print(newsgroups_train.data[0])\n",
    "print(newsgroups_train.target[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't define our own model, simply use pre-trained BERT model from Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "# Define \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define optimizer\n",
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Accessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, newsgroup):\n",
    "        self.newsgroup = newsgroup\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.newsgroup.filenames.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # return the sentence\n",
    "        i = self.newsgroup.data[idx]\n",
    "\n",
    "        # return one hot encoding of the label\n",
    "        #one_hot_label = np.array([0 for _ in range(len(available_classes))])\n",
    "        #one_hot_label[train_label[idx]] = 1\n",
    "        \n",
    "        # return the label directly\n",
    "        label = self.newsgroup.target[idx]\n",
    "\n",
    "        #return i, one_hot_label\n",
    "        return i, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of traning and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = SentenceDataset(newsgroups_train)\n",
    "test_dataset = SentenceDataset(newsgroups_test)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# Split training and validation set\n",
    "#train_len = int(0.6*len(dataset))\n",
    "#valid_len = len(dataset) - train_len\n",
    "#TrainData1, ValidationData1 = random_split(dataset,[train_len, valid_len])\n",
    "\n",
    "# Load into Iterator (each time get one batch)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True, num_workers=0)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True, num_workers=0)\n",
    "#train_loader = data.DataLoader(TrainData1, batch_size=batch_size, shuffle=True,drop_last=True, num_workers=0)\n",
    "#test_loader = data.DataLoader(ValidationData1, batch_size=batch_size, shuffle=True,drop_last=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Train with training set #\n",
    "###########################\n",
    "def train(model, iterator, optimizer, device):\n",
    "    \n",
    "    model.train()     # Enter Train Mode\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (sentences, labels) in enumerate(iterator):\n",
    "                \n",
    "        # tokenize the sentences\n",
    "        encoding = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        # move to GPU if necessary\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        # generate prediction\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # compute gradients and update weights\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record training losses\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # record processed data count\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # use Softmax to convert to probability\n",
    "        logits = outputs[1]\n",
    "        prob = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # take the index of the highest prob as prediction output\n",
    "        prediction = prob.max(1)[1]\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "\n",
    "    # print completed result\n",
    "    acc = 100.*correct/total\n",
    "    print('correct: %i  total: %i' % (correct, total))\n",
    "    print('train_loss: %s  test_acc: %f' % (train_loss, acc))\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "#############################\n",
    "# Validate with testing set #\n",
    "#############################\n",
    "def test(model, iterator, optimizer, device):\n",
    "\n",
    "    model.eval()     # Enter Evaluation Mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sentences, labels) in enumerate(iterator):\n",
    "            \n",
    "            # tokenize the sentences\n",
    "            encoding = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids']\n",
    "            attention_mask = encoding['attention_mask']\n",
    "            \n",
    "            # move to GPU if necessary\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            \n",
    "            # generate prediction\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            # record training losses\n",
    "            loss = outputs[0]\n",
    "            test_loss += loss\n",
    "            \n",
    "            # record processed data count\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # use Softmax to convert to probability\n",
    "            logits = outputs[1]\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # take the index of the highest prob as prediction output\n",
    "            prediction = prob.max(1)[1]\n",
    "            correct += prediction.eq(labels).sum().item()\n",
    "    \n",
    "    # print completed result\n",
    "    acc = 100.*correct/total\n",
    "    print('correct: %i  total: %i' % (correct, total))\n",
    "    print('test_loss: %f  test_acc: %f' % (test_loss, acc))\n",
    "    return test_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acutal execution:\n",
    "\n",
    "- Run `training()` and `test()` for `epoch` times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch 0 =====\n",
      "Training started ...\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    \n",
    "    print(\"===== Epoch %i =====\" % e)\n",
    "    \n",
    "    # training\n",
    "    print(\"Training started ...\")\n",
    "    train(model, train_loader, optimizer, device)\n",
    "\n",
    "    # validation testing\n",
    "    print(\"Testing started ...\")\n",
    "    test(model, test_loader, optimizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
